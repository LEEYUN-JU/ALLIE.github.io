---
layout: post
categories : [math]
comments: true
published: true

---

# [선형대수학] 1.9 Principal components and the Best Low Rank Matrix
#### Linear Algebra and Learning from Data

PCA (주성분 분석) : 행렬 A의 주성분인 특잇값을 사용하여 성분을 분석하는 것.

![사진 1](https://ifh.cc/g/P1c6wc.png)
 
행렬 A를 PCA를 통해 분해하는 식은 위와 같다. 이때, k는 행렬 A의 랭크이다.

위의 식에서 특잇값은 큰 수대로 나열되게 되는데 이것을 증명한 사람이 에카르트이며, 이때의 증명을 에카르트의 증명이라고 한다.

![enter image description here](https://ifh.cc/g/np3cjX.png)

에카르트가 증명한 내용은 위와 같다.

여기서 알아두어야 할 개념이 등장하는데, 세가지 노름에 대해서 배우게 된다.

![enter image description here](https://ifh.cc/g/d3Dgqr.png)

1. 스펙트럼 노름
2. 프로베니우스 노름
3. 핵 노름(대각합 노름)
  ___
 총 3가지가 등장하게 된다. 각각의 노름은 다른 값을 가지게 된다.
 
 스펙트럼 노름의 경우, 모든 x에 대해서 그 결과값인 Ax가 가장 길게 늘어난 것을 찾고, 벡터 Ax에 대한 norm을 구한 다음, 원래 벡터 x에 대한 norm으로 나누는 것이다. 
 ex) A = [2, 1], 스펙트럼 노름 = 2

 프로베니우스 노름의 경우, 모든 원소의 제곱의 합을 말한다. 값에는 루트를 씌워주게 된다.

 핵 노름의 경우, 대각합 노름 이라고도 불리우며, 어떤 행렬 A를 SVD로 분해하여 특잇값을 전부 더한 값이라고 볼 수 있다.
 ***
 #### 에카르드영 정리
 
![enter image description here](https://ifh.cc/g/GaLqrg.png)

Show 에 적혀있는것은 1,2,3,4 의 순서로 증명이 되고, 이는 곧 특잇값이 가장 큰 수로 나열 되어있다는 것...! 을 증명하는 것...!

에카르트 정리를
1. 프로베니우스 노름
2. 프로베니우스 노름에서의 에카르트-영
3. 프로베니우스  거리의 최소화

순으로 더 자세하게 설명하는 내용이 등장한다. 
(~~내용이 너무 어렵다고 느껴지는 경우,,, 그냥 대각 순으로 숫자가 크게 나열된다고 생각하자...~~)

***

#### PCA 분석
![enter image description here](https://ifh.cc/g/LJtzOb.png)
PCA? 어렵지 않다...

개념? 쉽게 이론적으로는 정방 행렬인 A가 존재한다고 가정할 때, 이 행렬의 주성분을 찾아 내는것을 SVD를 통해 찾아냈다면,

정방행렬이 아닌 B 행렬에 관해 주성분을 찾아내는 것이 PCA라고 생각하면 PCA가 좀 더 쉽게 느껴질까...?

원점인 (0,0) 을 가장 가깝게 지나는 직선을 찾아내는 방법은 특정 데이터를 주성분 분석을 통해 특잇값을 찾아낸 후, 가장 첫번째에 존재하는 값의 방향을 찾아내면 된다는 것으로 이해했다. 

이 책에서는 PCA를 통계학, 기하학, 선형 대수학의 관점에서 설명한다.
***
통계학 적인 관점에서 두 가지를 먼저 살펴보자.
>분산은 행렬 A와 A transpose의 대각성분이다.
>공분산은 행렬 A와 A transpose의 대각성분을 제외한 성분이다.
 
![enter image description here](https://ifh.cc/g/zPScn0.png)

위의 성질을 포함한 표본 공분산 행렬을 구하는 식은 위와 같다. 여기서 n은 데이터 열의 수를 말한다. 데이터의 크기 라고도 볼 수 있다.

표본 공분산 행렬을 구하게 되면, 위에서 부터 차례로 가장 큰 벡터에서 작은 벡터 순으로 나열 된다는 것을 의미하고, 이는 첫 벡터가 데이터에 가장 가까운 직선을 나타낸 다는 것을 의미한다.

***
