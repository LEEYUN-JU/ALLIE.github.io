---
layout: post
title: YOLO, You Only Look Once
comments: true
categories : [paper]
published: true

---

# YOLO: You Only Look Once

오늘은 YOLO 에 대한 이야기를 해보고자 한다. 
## Introduction
![enter image description here](https://ifh.cc/g/rfqFk0.png)
>(예시)

다음과 같은 그림이 존재한다고 치자.
이 그림에 사람이 어디에 위치해 있으며, 무엇을 하고 있는지, 만약 피한다면 언제 어떻게 피해야 하는지에 대한 판단을, 사람은 매우 순식간에 해낼 수 있다. 하지만 인공적으로 만들어진 모델은 그 판단을 하기까지 시간이 매우 오래 걸린다. 

이렇게 모델이 Object를 Detection 하는데는 총 두 가지 방법이 존재한다.
>하나는 one-stage 방식이고
>하나는 two-stage 방식이다

이름에서 느껴지듯이, two-stage는 말그대로 object detection하기까지 총 두 번의 과정을 거친 다는 것이고, one-stage는 한 번에 two-stage의 과정이 가능하다는 것이다. 각각에 장단점이 존재한다.
> <장점> 
> one-stage : 속도가 매우 빠르다
> two-stage: : 모델이 결과값을 내놓기 까지 매우 복잡하며, 시간도 오래 걸린다.
> <단점>
> one-stage : 정확도가 떨어진다
> two-stage: : 모듈별로 학습으르 시켜 주어야 하며, 속도가 느리다.

YOLO는 one-stage에 해당하는 모델이며, 기존 모델의 속도에 비해 매우 빠르며, 모델이 복잡하지 않고,  사용법도 간단하다.

## Unified Detection
![enter image description here](https://ifh.cc/g/lXpF01.jpg)

위의 그림은 욜로가 정확히 어떻게 이미지를 파악하는지를 보여주는 그림이다. 
우선 욜로는 입력으로 받은 그림을 사전에 지정된 그리드 값으로 자른다. 
본 논문에서 설정된 그리드 값(S)는 7이다. 

1. 입력받은 이미지를 7*7 그리드로 나눈다.
2. 각 그리드 별로 예측된 이미지의 boudning box를 그린다.( 본 논문에서 최대 2개로 설정함)
3. 예측된 각 obejct 별로 confidence score를 설정한다. (이 그림이 모델이 도출한 확률에 대해서 모델 스스로 얼마다 괜찮다고 하는지를 알 수 있는 값...?)
4. 이번에는 각 그리드 셀마다, Class probability map 을 그려서 그 그리드의 물체가 어느 클래스에 해당하는 지를 나타냅니다.
5. 2에서 그린 박스를 Non Max Suppresion  을 통해 값이 제일 높은 것만을 남기고 원본 위에 그립니다.

위의 결과를 거쳐서 최종적으로 7*7*(2*5+20) = 1470의 결과를 가지게 된다.

## Network Design
![enter image description here](https://ifh.cc/g/H3bplc.jpg)

다음은 YOLO의 네트워크 디자인에 대해서 이야기 해보고자 한다.

우선 본 논문에서 YOLO를 학습시키는데 사용한 데이터 셋은 PASCAL VOC데이터 셋을 이용해서 학습을 시켰다고 한다.

위의 데이터로 사진의 빨간 네모 부분을 pre-train 시켜준다.
총 24개의 convolution layer와, 2개의 fully connected layer로 구성되어 있다.

이미지가 선명한 경우보다 약간 흐릿한 경우가 물체와 배경의 segmentation이 더 잘 된다는 내용을 언급하면서, 이를 위해서 이미지 사이즈를 강제로 448x448로 확대시켰다는 내용이 나온다. 여기서, 이미지를 확장시켜서 input size를 강제로 늘려줬기 때문에, 모델이 연산하기 위한 1x1 reduction 레이어와 3x3 레이어를 추가시켜주었다는 내용이 등장한다.

> 빨간 부분에서 이미지의 전반적인 특징을 추출한다
> 기존 구조에서 yolo 모델에 새롭게 초록 부분을 추가한다. (이미지 사이즈 변경을 위한 부분)
> 어느 클래스로 분류될지에 대한 확률값과 바운딩 박스의 좌표를 예측하는 부분으로 되어 있다. (파란 부분)

## Activation function & Loss

![enter image description here](https://ifh.cc/g/7vlg1r.png)

Activation function은 다음과 같이 정의 되어 있다. 중간 레이어는 leaky ReLU를 사용하고, 마지막 레이어의 부분에서만 linear activation function을 사용한다고 되어 있다. 

![enter image description here](https://ifh.cc/g/Bw7oTV.png)

Loss Function 은 다음과 같이 정의된다. Sum-squared Error를 사용하고자 한다고 한다. 하지만, 몇 가지 문제가 존재한다고 한다.

1. bounding box의 위치 정보에 대한 loss 와 classification error 에 대한 weight 를 동일시 하는 것은 이상적이지 않다.
2. 모든 이미지의 모든 그리드 셀이 오브젝트를 포함하고 있다고 볼 수 없다. > 가중치를 둘 필요가 없는 그리드 셀이 존재할 수 있다.
3. 큰 바운딩 박스와 작은 바운딩 박스에 대해 동일한 weight 를 두는 것은 이상적이지 않다. 작은 바운딩 박스보다는 큰 바운딩 박스에 대한 편차가 더 적어야 한다.

### 최종 LOSS
![enter image description here](https://ifh.cc/g/lnBAZ7.png)
 
 최종 LOSS Function 은 다음과 같이 정의된다.
> 빨간 박스는 바운딩 박스에 관한 내용이고
> 주황색박스는 boudning box, width, height, localization error를 다룬다
> 초록색박스는 실제로 물체가 존재하는 bounding box 의 confidence error를 다룬다.
> 파란색 박스는 초록색 박스와 반대로 물체가 존재하지 않는 bounding box의 confidence error를 다룬다.
> 보라색 박스는 classification error로 객체를 포함한 바운딩 박스에 대해서만 계산한다.

여기서 등장하는 coord 값은 객체가 존재하는 바운딩 박스에 confidence loss 가중치를 늘리기 위해 설정된 매개변수로, 본 논문에서는 5. 로 설정되었다.
Noobj 값은 coord값과 반대로, 본 논문에서는 .5로 설정되었다.

![enter image description here](https://ifh.cc/g/PrQy0y.png)
 위의 인자가 의미하는 것은, 프로그램 적으로 if문에 해당한다고 생각하면 된다.

## 결론
![enter image description here](https://ifh.cc/g/BcjLS1.jpg)

다음은 분석 결과에 대한 내용이다. Fast R-CNN이 결과는 더 좋지만, 이 지표를 가져온 이유는, YOLO가 background 인식에 대한 오답률이 더 적기 때문이다. 
